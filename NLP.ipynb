{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwfZqZyvmFj_"
   },
   "source": [
    "# Introduction\n",
    "This notebook presents the development of two text sentiment classifiers: one leveraging Word2Vec embeddings and the other based on a Convolutional Neural Network (CNN) architecture.\n",
    "\n",
    "##About the Dataset\n",
    "The project uses a dataset containing consumer reviews of products purchased from e-commerce platforms. The dataset is publicly available on [Kaggle](https://www.kaggle.com/datasets/fredericods/ptbr-sentiment-analysis-datasets).\n",
    "\n",
    "This dataset is composed of several CSV files. With the exception of the concatenated.csv file, which combines all the other files, each file represents data from a different e-commerce platform. Among the available columns, the following will be used in this notebook:\n",
    "* review_text: the text containing the consumer's review of the product;\n",
    "* polarity: indicates whether the review is positive (value 1) or negative (value 0).\n",
    "\n",
    "## CNN neural network for text analysis\n",
    "Although originally designed for image analysis, Convolutional Neural Networks (CNNs) have proven to be useful for Natural Language Processing (*NLP*), as highlighted in the books Deep Learning for Natural Language Processing (RAAIJMAKERS, 2019) and Deep Learning for *NLP* and Speech Recognition (UDAY KAMATH et al., 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oP0e82XlgLjt"
   },
   "source": [
    "# Install libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMJd4iBjgJ3N",
    "outputId": "e0fae00e-7b29-496c-e8ab-27019727a74c"
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install spacy\n",
    "!pip install pycaret==3.0.0rc8 # I need install this version because of incompatibility of previous pycaret version with gensim and spacy\n",
    "!pip install tqdm\n",
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikX33A0rhQhy"
   },
   "source": [
    "# Downloading and Loading the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0Xx9lh3hO0_",
    "outputId": "68452a26-c06a-485c-c2ff-2184f54ba799"
   },
   "outputs": [],
   "source": [
    "!mkdir models\n",
    "!mkdir models/word2vec\n",
    "!curl -O http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s300.zip cbow_s300.zip\n",
    "!mv cbow_s300.zip models/word2vec\n",
    "!cd models/word2vec ; unzip -o -e  cbow_s300.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdalyJwJkWWz"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "def load_word2vec_model():\n",
    "    if(os.path.isfile('models/word2vec/cbow_s300')==False):\n",
    "        model=gensim.models.KeyedVectors.load_word2vec_format('models/word2vec/cbow_s300.txt')\n",
    "        model.save('models/word2vec/cbow_s300')\n",
    "    model_word2vec=gensim.models.KeyedVectors.load('models/word2vec/cbow_s300')\n",
    "    return model_word2vec\n",
    "\n",
    "model_word2vec=load_word2vec_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQagAIQhvYFp"
   },
   "source": [
    "# Database loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWTq-0Rxtr0H",
    "outputId": "bc1bcc6d-3715-4746-8e33-d523fa7960c1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67uBxffJvnVJ",
    "outputId": "da90cadf-0964-4baa-ecad-5d0afa98f592"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!cp /content/drive/MyDrive/Colab\\ Notebooks/braz_port_sent_ana_data/archive.zip data\n",
    "!cd data ; unzip -o -e archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf_XxRhWkGg_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "def get_dataset():\n",
    "    df=pd.read_csv(\"data/concatenated.csv\")\n",
    "    df=df[['review_text','polarity']].dropna()\n",
    "    data = list(zip(df['review_text'], df['polarity']))\n",
    "    dataset={ 'all_dataset':data,'max_rating':df['polarity'].max()}\n",
    "    return dataset\n",
    "\n",
    "def get_dataset_of_train_and_test():\n",
    "    data=get_dataset()\n",
    "    train, test = train_test_split(data['all_dataset'], test_size=0.3, random_state=0)\n",
    "    dataset_train_test={'train':train,'test':test,'max_rating':data['max_rating']}\n",
    "    return dataset_train_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68d5kocnlzrr",
    "outputId": "ddca36b9-4b91-43da-e72a-f79e2704e778"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, udf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "import os\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"]='spark_temp'\n",
    "\n",
    "MAX_MEMORY = \"10g\"\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf341')\\\n",
    "        .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "        .config(\"spark.driver.memory\", MAX_MEMORY)\\\n",
    "        .config(\"spark.local.dir\",os.environ[\"SPARK_LOCAL_DIRS\"])\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#spark.sparkContext.setLogLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O66pElbcl0EW"
   },
   "source": [
    "# Creating a Sentiment Classifier with Word2Vec\n",
    "## Creating a Dataset with the Word2Vec and Polarity Columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByzYvMZAeyBZ",
    "outputId": "0885d2df-5df1-4d81-824e-51b87d05a71e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import shutil\n",
    "import pathlib\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "def text_to_word2vec_sum(wor2vec_model,text):\n",
    "    ncolumns=wor2vec_model['ola'].shape[0]\n",
    "    word_vec=np.zeros(ncolumns,dtype=np.float)\n",
    "    words=gensim.utils.simple_preprocess(str(text),min_len=1,max_len=25, deacc=False)\n",
    "    for i in range(len(words)):\n",
    "        word=words[i]\n",
    "        if(word in wor2vec_model):\n",
    "            word_vec=wor2vec_model[word]+word_vec\n",
    "    return word_vec.tolist()\n",
    "\n",
    "\n",
    "dataset=get_dataset()\n",
    "\n",
    "def apply_wordtovec_to_dataset(dataset,model_word2vec):\n",
    "    shutil.rmtree(\"data/dataset_word2vec\",ignore_errors=True)\n",
    "    pathlib.Path(\"data/dataset_word2vec\").mkdir(parents=True, exist_ok=True)\n",
    "    buffer_to_save_df_in_dict={'polarity':[],'word2vec':[]}\n",
    "    number_of_subdf=0\n",
    "    for row in tqdm.tqdm(dataset):\n",
    "        array=text_to_word2vec_sum(model_word2vec,row[0])\n",
    "        buffer_to_save_df_in_dict['word2vec'].append(array)\n",
    "        buffer_to_save_df_in_dict['polarity'].append(row[1])\n",
    "        if(len(buffer_to_save_df_in_dict['polarity'])>1000):\n",
    "            sub_df_name=\"data/dataset_word2vec/\"+str(number_of_subdf)+'.parquet'\n",
    "            pd.DataFrame.from_dict(buffer_to_save_df_in_dict).to_parquet(sub_df_name)\n",
    "            number_of_subdf=number_of_subdf+1\n",
    "            buffer_to_save_df_in_dict={'polarity':[],'word2vec':[]}\n",
    "\n",
    "    if(len(buffer_to_save_df_in_dict['polarity'])>0):\n",
    "        sub_df_name=\"data/dataset_word2vec/\"+str(number_of_subdf)+'.parquet'\n",
    "        pd.DataFrame.from_dict(buffer_to_save_df_in_dict).to_parquet(sub_df_name)\n",
    "\n",
    "\n",
    "apply_wordtovec_to_dataset(dataset['all_dataset'],model_word2vec)\n",
    "\n",
    "\n",
    "#text_to_word2vec_sum_with_model=partial(text_to_word2vec_sum,model_word2vec)\n",
    "#dataget_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWnxTOfSbqfT"
   },
   "source": [
    "### Explanation of the Word2Vec Column Creation\n",
    "\n",
    "\n",
    "The Word2Vec column is derived from a preprocessing operation applied to the product review text. The preprocessing operation used was *Word2Vec sum*.\n",
    "\n",
    "###  Word2vec dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Tt5UE6NsgNL"
   },
   "outputs": [],
   "source": [
    "df_word2vec=spark.read.parquet('data/dataset_word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lXTOxRLb3Ok"
   },
   "source": [
    "## Creating a Sentiment Classifier for the Word2Vec Dataset\n",
    "### Using PyCaret to Determine the Best Classification Algorithm\n",
    "Load a sample of the Word2Vec dataset into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "nrs4sI_iu7fU",
    "outputId": "a8e6adc2-4a97-4c17-dc96-a61e24228fd0"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def pass_spark_df_word2vec_to_pandas_df(spark_df):\n",
    "    df_with_dataset_word2vec_in_dict={'polarity':[]}\n",
    "    firstIteration=True\n",
    "    with tqdm.tqdm(total=spark_df.count()) as pbar:\n",
    "        for iterator in spark_df.collect():\n",
    "            word2vec=iterator['word2vec']\n",
    "            if(firstIteration):\n",
    "                for i in range(len(word2vec)):\n",
    "                    df_with_dataset_word2vec_in_dict['col'+str(i)]=[]\n",
    "                firstIteration=False\n",
    "\n",
    "            for i in range(len(word2vec)):\n",
    "                df_with_dataset_word2vec_in_dict['col'+str(i)].append(word2vec[i])\n",
    "            df_with_dataset_word2vec_in_dict['polarity'].append(iterator['polarity'])\n",
    "            pbar.update(1)\n",
    "    return pd.DataFrame.from_dict(df_with_dataset_word2vec_in_dict)\n",
    "\n",
    "df_word2vec_pd=pass_spark_df_word2vec_to_pandas_df(df_word2vec.sample(0.04))\n",
    "df_word2vec_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F2YeS-2cWv5"
   },
   "source": [
    "\n",
    "\n",
    "Apply PyCaret to the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4f66dda60f2e490787e5b1f15063892e",
      "12c748e965e84a6199bc7ea0622c8add",
      "019e8bb15b44474cac630508f697d3fc",
      "3824de32309e434e9555a8e204ce5cd7",
      "a01bf043e14948beb9626c8c7fbe7d64",
      "9e09dd1224c44632b576b464e107c902",
      "0cffa1ba8f2a43bba13c6d95b7e529e5",
      "28e9d63bfc6245aaa3985f59ab77cee4",
      "c13bfad3016b4e0ab29d71da8ac50abe",
      "5603d6eadee242b1ba10ecea32b0934e",
      "689c339542624441afb15cdc3cb807c7"
     ]
    },
    "id": "a0yYhr9aCIhj",
    "outputId": "f444e3d8-56b9-4e8e-92d7-13b52738540a"
   },
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "clf1 = setup(data = df_word2vec_pd, target = 'polarity')\n",
    "compare_models(exclude=['knn','gbc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ12DbXZbrg_"
   },
   "source": [
    "Based on the PyCaret data, the best classification algorithm is the *Light Gradient Boosting Machine*. The closest equivalent in PySpark is Gradient Boosting. PySpark will be used to create the final machine learning model, as the dataset exceeds the available RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgbAtg7wWauI"
   },
   "source": [
    "### Creating the Final Classifier Using the PySpark Machine Learning Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sHbFaKzihWW"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors,VectorUDT\n",
    "convert_array_to_vector_udf= udf(lambda x:  Vectors.dense(list(x)),VectorUDT())\n",
    "df_word2vec_mlib=df_word2vec.withColumn('word2vec_vector',convert_array_to_vector_udf(col('word2vec')))\n",
    "\n",
    "df_train, df_test = df_word2vec_mlib.randomSplit(weights=[0.8,0.2], seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "FAC51Bkk9q85",
    "outputId": "bd666131-68d6-4341-e854-5caec60ab5b1"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbc=GBTClassifier(featuresCol='word2vec_vector', labelCol='polarity')\n",
    "model = gbc.fit(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQJX6hJNi39s",
    "outputId": "c047fb30-a013-4f0e-8208-613484cd81c5"
   },
   "outputs": [],
   "source": [
    "gb_predictions = model.transform(df_test)\n",
    "gb_predictions.write.parquet('predictions_wordvec_test.parquet')\n",
    "gb_predictions_train = model.transform(df_train)\n",
    "gb_predictions_train.write.parquet('predictions_wordvec_train.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjcBlcJaWauL"
   },
   "outputs": [],
   "source": [
    "gb_predictions=df_word2vec=spark.read.parquet('predictions_wordvec_test.parquet')\n",
    "gb_predictions_train=df_word2vec=spark.read.parquet('predictions_wordvec_train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faAg3lhbWauM"
   },
   "source": [
    "#### Analyzing Prediction Quality on the Test Set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3z3qTMuAWauN",
    "outputId": "3418f591-ed05-4280-87f2-655763d53d58"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='polarity',rawPredictionCol='probability')\n",
    "auroc = evaluator.evaluate(gb_predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(\"Area under ROC Curve: {:.4f}\".format(auroc))\n",
    "\n",
    "y_true = gb_predictions.select(['polarity']).collect()\n",
    "y_pred = gb_predictions.select(['prediction']).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBHR7CbtmNKQ"
   },
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-Eh1TfeWauO",
    "outputId": "a9fac0bf-de20-4a2f-c56e-4903bdb1a3ed"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_true,y_pred),columns=[['predito','predito'],['Polaridade 0','Polaridade 1']], index=[['real','real'],['Polaridade 0','Polaridade 1']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPE-4YAIWauP"
   },
   "source": [
    "The model shows a reasonably good AUC. However, the confusion matrix indicates a high number of false positives. This suggests that only items with a high predicted probability of being positive should be classified as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4jgfFbfYWZw"
   },
   "source": [
    "# Creating a Sentiment Classifier Using Neural Networks\n",
    "## Creating data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aw7kSEilYzIh"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.layers import Embedding, Input,Concatenate\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, df,max_ratings,batch_size,vocab_size,nwords, shuffle=True):\n",
    "        self.__df = df\n",
    "        self.__batch_size = batch_size\n",
    "        self.__shuffle = shuffle\n",
    "        self.__vocab_size=vocab_size\n",
    "        self.__nwords=nwords\n",
    "        self.__max_ratings=max_ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.__df) / self.__batch_size))\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return int(self.__vocab_size)\n",
    "\n",
    "    def get_nwords(self):\n",
    "        return int(self.__nwords)\n",
    "\n",
    "    def get_nclasses(self):\n",
    "        return int(self.__max_ratings)+1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        df = self.__df[index*self.__batch_size:(index+1)*self.__batch_size]\n",
    "        x=np.zeros((len(df),self.__nwords),dtype=np.int32)\n",
    "        y=np.zeros((len(df),int(self.__max_ratings)+1),dtype=np.float)\n",
    "        for i in range(len(df)):\n",
    "            row=df[i]\n",
    "            text=row[0]\n",
    "            text_np = self.__data_generation(text)\n",
    "            x[i,:]=text_np\n",
    "            y[i,int(row[1])]=1\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.__shuffle == True:\n",
    "            random.shuffle(self.__df)\n",
    "\n",
    "    def __data_generation(self, text):\n",
    "\n",
    "        text_enc=one_hot(text,self.__vocab_size)\n",
    "        padded_reviews = pad_sequences([text_enc],maxlen=self.get_nwords(),padding='post')[0]\n",
    "        return padded_reviews\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr2nY0Y56zkV"
   },
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWoOqTJfYSTP"
   },
   "outputs": [],
   "source": [
    "def create_model_cnn(datagen:DataGenerator):\n",
    "\n",
    "    number_of_classes=datagen.get_nclasses()\n",
    "    model = Sequential()\n",
    "    deep_inputs = Input(shape=(datagen.get_nwords(),))\n",
    "    emb=Embedding(input_dim=datagen.get_vocab_size(),output_dim=128)(deep_inputs)\n",
    "\n",
    "\n",
    "    conv1=Conv1D(32,3,padding='same',strides=1,activation='relu')(emb)\n",
    "    maxPool1=MaxPool1D(pool_size=2,strides=2)(conv1)\n",
    "    drop1=Dropout(0.2)(maxPool1)\n",
    "\n",
    "    conv2=Conv1D(32,2,padding='same',strides=1,activation='relu')(drop1)\n",
    "    maxPool2=MaxPool1D(pool_size=2,strides=2)(conv2)\n",
    "    drop2=Dropout(0.2)(maxPool2)\n",
    "\n",
    "    conv3=Conv1D(32,2,padding='same',strides=1,activation='relu')(drop2)\n",
    "    maxPool3=MaxPool1D(pool_size=2,strides=2)(conv3)\n",
    "    drop3=Dropout(0.2)(maxPool3)\n",
    "\n",
    "    conv4=Conv1D(32,2,padding='same',strides=1,activation='relu')(drop3)\n",
    "    maxPool4=MaxPool1D(pool_size=2,strides=2)(conv4)\n",
    "    drop4=Dropout(0.2)(maxPool4)\n",
    "\n",
    "    conv5=Conv1D(32,2,padding='same',strides=1,activation='relu')(drop4)\n",
    "    maxPool5=MaxPool1D(pool_size=2,strides=2)(conv5)\n",
    "    drop5=Dropout(0.2)(maxPool5)\n",
    "\n",
    "    conv6=Conv1D(32,2,padding='same',strides=1,activation='relu')(drop5)\n",
    "    maxPool6=MaxPool1D(pool_size=2,strides=2)(conv6)\n",
    "    drop6=Dropout(0.2)(maxPool6)\n",
    "\n",
    "    fla=Flatten()(drop6)\n",
    "    den1=Dense(2000, activation='relu')(fla)\n",
    "    drop_den1=Dropout(0.2)(den1)\n",
    "    den2=Dense(2000, activation='relu')(drop_den1)\n",
    "    drop_den2=Dropout(0.2)(den2)\n",
    "    out=Dense(number_of_classes, activation='softmax')(drop_den2)\n",
    "    model=Model(inputs=deep_inputs, outputs=out)\n",
    "    # choosing Adam optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    # we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                   optimizer=optimizer,\n",
    "                   metrics='accuracy')\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKdA4xU27F3T"
   },
   "source": [
    "## Creating a Classifier Using a CNN Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6RFgEvdZQga",
    "outputId": "72712184-c027-4ce4-c0f8-ddbfda83b097"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "vocab_size = 50000\n",
    "datasets=get_dataset_of_train_and_test()\n",
    "datagen_train=DataGenerator(datasets['train'],max_ratings=datasets['max_rating'],batch_size=batch_size,vocab_size= vocab_size,nwords=256)\n",
    "datagen_test=DataGenerator(datasets['test'],max_ratings=datasets['max_rating'],batch_size=batch_size,vocab_size= vocab_size,nwords=256)\n",
    "model = create_model_cnn(datagen_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLpurlk8f3Vr",
    "outputId": "2ba6c6b8-9082-4446-b921-daf28d15b71e"
   },
   "outputs": [],
   "source": [
    "number_of_epochs = 5\n",
    "history = model.fit(datagen_train,epochs=number_of_epochs,validation_data=datagen_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7-K-umF7Biu"
   },
   "source": [
    "### Analyzing Prediction Quality on the Test Set\n",
    "\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1I3bvVFyVvY",
    "outputId": "d36aaef7-6950-4ccc-afd0-608d3a7d1266"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_test=[]\n",
    "y_test_prob=[]\n",
    "\n",
    "for i in tqdm.tqdm(range(datagen_test.__len__())):\n",
    "    x,y=datagen_test.__getitem__(i)\n",
    "    prob=model.predict(x,verbose=False)\n",
    "    for j in range(len(prob)):\n",
    "        y_test.append(int(y[j][1]))\n",
    "        y_test_prob.append(prob[j,1])\n",
    "roc_auc_score(y_test, y_test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWPrJ_kl7x-W"
   },
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "wGP0oh8T4z1I",
    "outputId": "2898733e-4057-455a-b1bb-c6780f8cd2e3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred=[]\n",
    "for x in y_test_prob:\n",
    "    if(x>0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test,y_pred),columns=[['predito','predito'],['Polaridade 0','Polaridade 1']], index=[['real','real'],['Polaridade 0','Polaridade 1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojEMrhxz-S3N"
   },
   "source": [
    "By analyzing the metrics, it can be concluded that the model has a good AUC, close to 1, which is the maximum value. The confusion matrix shows that the model achieves a good accuracy (93%) along with strong precision (94%) and recall (97%) values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOov9XIUApLv"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook compares two approaches for creating a sentiment classifier for text data. The first approach used gradient boosting to build a classifier that processes texts with Word2Vec sum. The second approach employed a CNN neural network. Based on the results, the second approach demonstrated better performance.\n",
    "\n",
    "For future work, it would be interesting to assess the computational cost of both models when making predictions. This is particularly important when deploying machine learning algorithms in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXFJ5RPtqXjz"
   },
   "source": [
    "# Bibliographic references\n",
    "\n",
    "RAAIJMAKERS, S. Deep Learning for Natural Language Processing. [s.l.] Manning Publications Company, 2019.\n",
    "\n",
    "UDAY KAMATH et al. Deep Learning for NLP and Speech Recognition. Cham: Springer International Publishing, 2019."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xwfZqZyvmFj_"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "019e8bb15b44474cac630508f697d3fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28e9d63bfc6245aaa3985f59ab77cee4",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c13bfad3016b4e0ab29d71da8ac50abe",
      "value": 53
     }
    },
    "0cffa1ba8f2a43bba13c6d95b7e529e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12c748e965e84a6199bc7ea0622c8add": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e09dd1224c44632b576b464e107c902",
      "placeholder": "​",
      "style": "IPY_MODEL_0cffa1ba8f2a43bba13c6d95b7e529e5",
      "value": "Processing: 100%"
     }
    },
    "28e9d63bfc6245aaa3985f59ab77cee4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3824de32309e434e9555a8e204ce5cd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5603d6eadee242b1ba10ecea32b0934e",
      "placeholder": "​",
      "style": "IPY_MODEL_689c339542624441afb15cdc3cb807c7",
      "value": " 53/53 [58:41&lt;00:00, 24.59s/it]"
     }
    },
    "4f66dda60f2e490787e5b1f15063892e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_12c748e965e84a6199bc7ea0622c8add",
       "IPY_MODEL_019e8bb15b44474cac630508f697d3fc",
       "IPY_MODEL_3824de32309e434e9555a8e204ce5cd7"
      ],
      "layout": "IPY_MODEL_a01bf043e14948beb9626c8c7fbe7d64"
     }
    },
    "5603d6eadee242b1ba10ecea32b0934e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "689c339542624441afb15cdc3cb807c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e09dd1224c44632b576b464e107c902": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a01bf043e14948beb9626c8c7fbe7d64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "c13bfad3016b4e0ab29d71da8ac50abe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
